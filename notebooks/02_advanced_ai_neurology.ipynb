{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKu8UFoCICBgG/Zs8gN/Jg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdelleani/neuro-next-bootcamp/blob/main/notebooks/02_advanced_ai_neurology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yGUtddnciENO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "930dff9d-416b-49f3-f849-a8770e5d5faf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'neuro-next-bootcamp'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Counting objects: 100% (128/128), done.\u001b[K\n",
            "remote: Compressing objects: 100% (112/112), done.\u001b[K\n",
            "remote: Total 128 (delta 56), reused 43 (delta 9), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (128/128), 9.12 MiB | 9.69 MiB/s, done.\n",
            "Resolving deltas: 100% (56/56), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mdelleani/neuro-next-bootcamp.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applicazioni Avanzate dell'IA in Neurologia: Dalla Segmentazione Immagini ai LLM üß†\n",
        "\n",
        "**Sessione Interattiva | Parte II: Applicazioni Pratiche**\n",
        "\n",
        "* **Docente:** M. Delleani\n",
        "* **Orario:** 18:00 - 19:15\n",
        "\n",
        "---\n",
        "\n",
        "Benvenuti alla seconda parte della nostra sessione interattiva!\n",
        "\n",
        "Mentre il titolo originale della sessione era focalizzato sulla generazione di dati sintetici, per questa demo pratica abbiamo scelto di esplorare due aree di frontiera dell'Intelligenza Artificiale con un impatto immediato e tangibile in Neurologia, pi√π facilmente dimostrabili in un contesto di sessione interattiva:\n",
        "\n",
        "\n",
        "1.  **Large Language Models (LLM) per l'Analisi Testuale:** Come i modelli di linguaggio possono elaborare e interpretare testo medico (es. cartelle cliniche, articoli scientifici) per estrarre informazioni chiave, riassumere o rispondere a domande.\n",
        "2.  **Segmentazione di Immagini Mediche:** Come l'IA pu√≤ aiutare a identificare e delineare automaticamente strutture o patologie (es. lesioni, tumori, regioni cerebrali) nelle immagini diagnostiche (RM, TC).\n",
        "\n",
        "Queste applicazioni mostrano la versatilit√† e il potenziale dell'IA nel supportare la diagnosi, la ricerca e la gestione clinica in neurologia.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "pUIphWGwiRWI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Large Language Models (LLM) per l'Analisi Testuale in Neurologia\n",
        "I Large Language Models (LLM) sono modelli di IA addestrati su enormi quantit√† di testo per comprendere, generare e manipolare il linguaggio umano. Hanno rivoluzionato il modo in cui interagiamo con l'informazione e stanno mostrando un potenziale immenso anche nel settore medico.\n"
      ],
      "metadata": {
        "id": "jKep_WWWnEUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 1.1. Applicazioni degli LLM in Medicina e Neurologia\n",
        "\n",
        "- Riassunto di Cartelle Cliniche/Articoli: Estrarre i punti chiave da lunghi testi.\n",
        "\n",
        "- Estrazione di Informazioni: Identificare sintomi, diagnosi, farmaci, dosaggi da testi non strutturati.\n",
        "\n",
        "- Supporto Decisionale Clinico: Fornire informazioni basate sull'evidenza da banche dati mediche (con cautela).\n",
        "\n",
        "- Generazione di Rapporti: Aiutare a redigere referti o documentazione.\n",
        "Analisi di Testi Scientifici: Accelerare la revisione della letteratura.\n"
      ],
      "metadata": {
        "id": "mQXFuiCanSmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 1.2. Interagire con LLM Generali e Specializzati\n",
        "\n",
        "Avremo la possibilit√† di interagire con un server di inferenza ad alte prestazioni per LLMche ci permetter√† di testare rapidamente modelli di linguaggio.\n",
        "\n",
        "Useremo un approccio basato su API (Application Programming Interface), simulando una chiamata a un servizio esterno che ospita il modello."
      ],
      "metadata": {
        "id": "lgna9NzQnUiv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Esempio testo in Neurologia**"
      ],
      "metadata": {
        "id": "QUN4lC1rnn87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testo_clinico_neurologia = \"\"\"\n",
        "Paziente M.D., 72 anni, ricoverato per peggioramento delle difficolt√† cognitive iniziate 6 mesi fa,\n",
        "caratterizzate principalmente da deficit di memoria episodica, disorientamento temporale e spaziale.\n",
        "L'esame neurologico evidenzia lieve disprassia costruttiva. La RM cerebrale mostra atrofia ippocampale\n",
        "bilaterale e una modesta componente vascolare. CSF positivo per biomarker di Alzheimer (ridotta AŒ≤42, aumentata p-tau).\n",
        "Diagnosi provvisoria: Demenza di tipo Alzheimer con componente vascolare. Iniziato trattamento con Donepezil 5mg/die.\n",
        "Si programma follow-up tra 3 mesi.\n",
        "\"\"\"\n",
        "\n",
        "testo_articolo_neurologia = \"\"\"\n",
        "Uno studio recente ha esplorato l'efficacia di un nuovo anticorpo monoclonale, Aducanumab,\n",
        "nel ridurre le placche amiloidi nel cervello di pazienti affetti da Alzheimer in fase precoce.\n",
        "I risultati preliminari indicano una riduzione significativa del carico di amiloide,\n",
        "tuttavia, la correlazione con il miglioramento clinico rimane oggetto di dibattito,\n",
        "con alcuni pazienti che riportano eventi avversi come ARIA-E. La ricerca √® stata pubblicata\n",
        "su \"Nature Neuroscience\".\n",
        "\"\"\"\n",
        "\n",
        "print(\"Testo Clinico di Esempio:\")\n",
        "print(testo_clinico_neurologia)\n",
        "print(\"\\nTesto Articolo Scientifico di Esempio:\")\n",
        "print(testo_articolo_neurologia)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fB4CH_qbmw-D",
        "outputId": "b6522f51-0936-44d4-bcb8-fa512744baf0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testo Clinico di Esempio:\n",
            "\n",
            "Paziente M.D., 72 anni, ricoverato per peggioramento delle difficolt√† cognitive iniziate 6 mesi fa,\n",
            "caratterizzate principalmente da deficit di memoria episodica, disorientamento temporale e spaziale.\n",
            "L'esame neurologico evidenzia lieve disprassia costruttiva. La RM cerebrale mostra atrofia ippocampale\n",
            "bilaterale e una modesta componente vascolare. CSF positivo per biomarker di Alzheimer (ridotta AŒ≤42, aumentata p-tau).\n",
            "Diagnosi provvisoria: Demenza di tipo Alzheimer con componente vascolare. Iniziato trattamento con Donepezil 5mg/die.\n",
            "Si programma follow-up tra 3 mesi.\n",
            "\n",
            "\n",
            "Testo Articolo Scientifico di Esempio:\n",
            "\n",
            "Uno studio recente ha esplorato l'efficacia di un nuovo anticorpo monoclonale, Aducanumab,\n",
            "nel ridurre le placche amiloidi nel cervello di pazienti affetti da Alzheimer in fase precoce.\n",
            "I risultati preliminari indicano una riduzione significativa del carico di amiloide,\n",
            "tuttavia, la correlazione con il miglioramento clinico rimane oggetto di dibattito,\n",
            "con alcuni pazienti che riportano eventi avversi come ARIA-E. La ricerca √® stata pubblicata\n",
            "su \"Nature Neuroscience\".\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Funzione per Chiamare un LLM (via API)**\n",
        "\n",
        "Simuleremo una chiamata API al tuo server vLLM. Dovrai sostituire YOUR_VLLM_ENDPOINT e YOUR_MODEL_NAME con i dettagli reali del tuo setup vLLM."
      ],
      "metadata": {
        "id": "_h5jR0rLoZI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd neuro-next-bootcamp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok43rG_F9QXa",
        "outputId": "7cc56074-a3c2-4ee1-f2df-7db32ba64a35"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/neuro-next-bootcamp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testo_referto = testo_clinico_neurologia\n",
        "informazioni_da_estrarre = \"motivo vista, diagnosi, sintomi, trattamento consigliato, imaging, anamnesi\""
      ],
      "metadata": {
        "id": "v2bb_p1C-awe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from notebooks.utils import ask_generalist_llm, ask_specialized_llm\n",
        "import asyncio\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"Sei un assistente medico neurologo.\"}]},\n",
        "    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": f\"Dato il Referto: {testo_referto}\\n\\nEstrai: {informazioni_da_estrarre}\"}]}\n",
        "]\n",
        "\n",
        "# Run one or the other\n",
        "response = await ask_generalist_llm(messages)\n",
        "\n",
        "print(\"Risposta del modello generalista:\\n\", response)\n"
      ],
      "metadata": {
        "id": "RdYSZ__BnrFh",
        "outputId": "43f86e27-78c3-487f-9580-d7e31f9a940c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'notebooks.utils'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-3710816521.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnotebooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mask_generalist_llm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mask_specialized_llm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m messages = [\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"system\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Sei un assistente medico neurologo.\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'notebooks.utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_specialized = await ask_specialized_llm(messages)\n",
        "\n",
        "print(\"Risposta del modello specializzato:\\n\", response_specialized)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "9DOLzH_5950V",
        "outputId": "cbc6274d-49bc-4dc5-9434-e0b7327a4ead"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ask_specialized_llm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-3683232788.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse_specialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mask_specialized_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Risposta del modello specializzato:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_specialized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ask_specialized_llm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Few-shot prompting\n",
        "\n",
        "Instruire il modello a rispondere come vogliamo dandogli alcuni esempi"
      ],
      "metadata": {
        "id": "TYLzAuCOEb0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dPSR3a8mJkun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. Prompt few-shot + nuovo referto\n",
        "few_shot_examples = \"\"\"\n",
        "Referto:\n",
        "Paziente maschio di 52 anni con cefalea intensa, nausea e rigidit√† nucale. RM encefalo mostra emorragia subaracnoidea. Inizia trattamento in neurochirurgia. Diagnosi: emorragia subaracnoidea.\n",
        "\n",
        "Risposta:\n",
        "Diagnosi: Emorragia subaracnoidea\n",
        "Sintomi principali: Cefalea intensa, nausea, rigidit√† nucale\n",
        "Trattamento consigliato: Osservazione e valutazione neurochirurgica\n",
        "Imaging anomalo: s√¨, emorragia subaracnoidea alla RM\n",
        "\n",
        "---\n",
        "\n",
        "Referto:\n",
        "Donna di 74 anni con progressiva perdita di memoria, disorientamento temporale e alterazioni comportamentali. TC encefalo: atrofia diffusa. Diagnosi probabile: demenza di Alzheimer. Inizia trattamento con donepezil.\n",
        "\n",
        "Risposta:\n",
        "Diagnosi: Demenza di Alzheimer (probabile)\n",
        "Sintomi principali: Perdita di memoria, disorientamento, alterazioni comportamentali\n",
        "Trattamento consigliato: Donepezil\n",
        "Imaging anomalo: s√¨, atrofia diffusa alla TC\n",
        "\n",
        "---\n",
        "\n",
        "Referto:\n",
        "Paziente di 67 anni giunge in PS per insorgenza improvvisa di emiparesi destra e disartria. TC encefalo negativa. RM encefalo mostra lesione ischemica recente in territorio lenticolo-capsulare sinistro. Inizia trattamento con ASA e statine. Diagnosi: ictus ischemico in paziente ipertesa.\n",
        "\n",
        "Risposta:\n",
        "\"\"\"\n",
        "\n",
        "esempi_few_shot = few_shot_examples.strip()\n",
        "esempi_few_shot"
      ],
      "metadata": {
        "id": "f2CTiQDv6Qul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"Sei un assistente medico neurologo.\"}]},\n",
        "    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": f\"Ecco alcuni esempi:\\n {esempi_few_shot}\\n\\nDato il Referto: {testo_referto}\\n\\nEstrai: {informazioni_da_estrarre}\"}]},\n",
        "]\n",
        "\n",
        "# Run one or the other\n",
        "response = await ask_generalist_llm(messages)\n",
        "\n",
        "print(\"Risposta del modello generalista:\\n\", response)"
      ],
      "metadata": {
        "id": "pDBDhqjzP-4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run one or the other\n",
        "response = await ask_specialized_llm(messages)\n",
        "\n",
        "print(\"Risposta del modello generalista:\\n\", response)"
      ],
      "metadata": {
        "id": "TSyZ3JjSGU2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Forziamo le strutturazioni dell'output"
      ],
      "metadata": {
        "id": "R-ppdbf0GuyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "from enum import Enum\n",
        "\n",
        "\n",
        "class Diagnosi(str, Enum):\n",
        "    alzheimer = \"alzheimer\"\n",
        "    mielodisplasia = \"mielodisplasia\"\n",
        "    leucemia = \"leucemia\"\n",
        "\n",
        "class PatientDescription(BaseModel):\n",
        "    name: str\n",
        "    age: str\n",
        "    diagnosi: Diagnosi\n",
        "    sintomi: str\n",
        "\n",
        "json_schema = PatientDescription.model_json_schema()\n",
        "\n",
        "\n",
        "response_format = {\n",
        "                    \"type\": \"json_schema\",\n",
        "                    \"json_schema\": {\n",
        "                        \"name\": \"car-description\",\n",
        "                        \"schema\": PatientDescription.model_json_schema()\n",
        "                    },\n",
        "                  }\n"
      ],
      "metadata": {
        "id": "T1tNQjnZT2mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"Sei un assistente medico neurologo.\"}]},\n",
        "    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": f\"Dato il Referto: {testo_referto}\\n\\nEstrai: {informazioni_da_estrarre}\"}]},\n",
        "]\n",
        "\n",
        "# Run one or the other\n",
        "response = await ask_generalist_llm(messages, response_format)\n",
        "\n",
        "print(\"Risposta del modello generalista:\\n\", response)"
      ],
      "metadata": {
        "id": "uyj2dtQVUlV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "qmlN0pzoWSgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vJZpqDEIQVIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MedGemma: multimodalit√† (immagini e testo)"
      ],
      "metadata": {
        "id": "ixoBfQRMIJgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from IPython.display import Image as IPImage, display, Markdown\n",
        "\n",
        "prompt = \"Descrivi questa MRI\"  # @param {type: \"string\"}\n",
        "\n",
        "# Image attribution: Stillwaterising, CC0, via Wikimedia Commons\n",
        "image_url = \"https://alzheimersnewstoday.com/wp-content/uploads/2014/10/WhiteMatterHyperintensities.jpg\" # \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\"  # @param {type: \"string\"}\n",
        "! wget -nc -q {image_url}\n",
        "image_filename = os.path.basename(image_url)\n",
        "image = Image.open(image_filename)"
      ],
      "metadata": {
        "id": "iusGDelOgHIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(IPImage(filename=image_filename, height=300))\n"
      ],
      "metadata": {
        "id": "7OUJ8whKuE6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_instruction = \"Sei un esperto neurologo.\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": [{\"type\": \"text\", \"text\": system_instruction}]\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": prompt},\n",
        "            {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n",
        "        ]\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "_jFhVg3KoWbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = await ask_specialized_llm(messages, response_format)\n"
      ],
      "metadata": {
        "id": "v9GQhVttVy8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if response_text:\n",
        "\n",
        "  display(Markdown(f\"---\\n\\n**[ User ]**\\n\\n{prompt}\"))\n",
        "  display(IPImage(filename=image_filename, height=300))\n",
        "  display(Markdown(f\"---\\n\\n**[ MedGemma ]**\\n\\n{response_text}\\n\\n---\"))"
      ],
      "metadata": {
        "id": "MLV_8UA2opeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2. Segmentazione di Immagini in Neurologia: Identificare e Delineare\n",
        "\n",
        "La **segmentazione di immagini** √® una tecnica di visione artificiale che consiste nel suddividere un'immagine in regioni o oggetti con propriet√† simili. In medicina, √® fondamentale per:\n",
        "\n",
        "* **Quantificazione:** Misurare il volume di lesioni, tumori o atrofia cerebrale.\n",
        "* **Navigazione Chirurgica:** Delineare le aree di interesse per interventi precisi.\n",
        "* **Diagnosi:** Aiutare a identificare e localizzare patologie."
      ],
      "metadata": {
        "id": "2MLqvpvGeaJU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2.1. MedSAM: Un Modello Fondamentale per la Segmentazione Medica\n",
        "\n",
        "**MedSAM (Medical Segment Anything Model)** √® un esempio di \"foundation model\" adattato al dominio medico. Nasce dal successo del modello Segment Anything Model (SAM) di Meta AI, addestrato su una quantit√† enorme di immagini generiche. MedSAM √® stato poi specializzato per riconoscere e segmentare una vasta gamma di strutture anatomiche e patologie nelle immagini mediche con notevole accuratezza, anche con pochi esempi o con input minimi (es. un singolo punto).\n",
        "\n",
        "**Nota importante per la demo:** L'esecuzione di MedSAM su immagini complesse pu√≤ richiedere risorse computazionali significative (GPU dedicate e RAM). Per questa sessione, dimostreremo il concetto: potremmo mostrare l'output pre-calcolato su un'immagine di esempio, oppure, se le risorse di Colab lo permettono in tempo reale per un esempio semplificato, eseguiremo una versione pi√π leggera o un modello simile.\n",
        "\n",
        "#### **Esempio Concettuale di Segmentazione**\n",
        "\n",
        "Simuleremo il caricamento di un'immagine di risonanza magnetica cerebrale e visualizzeremo un esempio di segmentazione di una lesione.\n"
      ],
      "metadata": {
        "id": "uWkl5AVLpKGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# --- Parte 1: Download e Visualizzazione di un'Immagine Medica di Esempio ---\n",
        "# Useremo un'immagine di esempio disponibile pubblicamente o genereremo una dummy.\n",
        "# Per una demo reale, potresti avere un'immagine di RM cerebrale con una lesione.\n",
        "\n",
        "# Funzione per scaricare un'immagine\n",
        "def download_image(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status() # Lancia un errore per risposte http errate\n",
        "        img = Image.open(BytesIO(response.content)).convert(\"L\") # Converti in scala di grigi\n",
        "        return np.array(img)\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante il download o l'apertura dell'immagine: {e}\")\n",
        "        # Genera un'immagine dummy se il download fallisce\n",
        "        print(\"Generazione di un'immagine di esempio sintetica...\")\n",
        "        dummy_img = np.zeros((128, 128), dtype=np.uint8)\n",
        "        # Aggiungi una forma semplice per simulare una struttura\n",
        "        dummy_img[30:60, 30:60] = 150 # Regione grigia\n",
        "        dummy_img[70:90, 70:90] = 200 # Regione pi√π chiara\n",
        "        return dummy_img"
      ],
      "metadata": {
        "id": "NbI77jW0pANs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# URL di un'immagine di esempio (puoi sostituirla con una tua immagine medicale)\n",
        "# Questa √® un'immagine generica di un cervello.\n",
        "image_url = 'https://www.learnhaem.com/wp-content/uploads/2020/02/aml-m1-1024x691.png' # @param\n",
        "# Puoi anche caricare un'immagine direttamente su Colab e usarla:\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# img_path = list(uploaded.keys())[0]\n",
        "# original_image = np.array(Image.open(img_path).convert(\"L\"))\n",
        "\n",
        "original_image = download_image(image_url)\n",
        "\n",
        "if original_image is not None:\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(original_image, cmap='gray')\n",
        "    plt.title('Immagine RM Cerebrale di Esempio')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# --- Parte 2: Simulazione Concettuale della Segmentazione ---\n",
        "# Per una demo live con MedSAM, dovresti installare le librerie MedSAM e caricarne un checkpoint.\n",
        "# Dato il tempo limitato e le risorse, mostreremo il CONCETTO di mascheramento.\n",
        "\n",
        "# Creiamo una maschera di segmentazione FITTIZIA per l'esempio\n",
        "# Questa maschera simula la segmentazione di una lesione o di una struttura specifica.\n",
        "segmentation_mask = np.zeros_like(original_image, dtype=np.uint8)\n",
        "# Simula una lesione nella parte superiore destra dell'immagine\n",
        "mask_center_x, mask_center_y = int(original_image.shape[1] * 0.7), int(original_image.shape[0] * 0.3)\n",
        "mask_radius = int(min(original_image.shape) * 0.15)\n",
        "Y, X = np.ogrid[:original_image.shape[0], :original_image.shape[1]]\n",
        "dist_from_center = np.sqrt((X - mask_center_x)**2 + (Y - mask_center_y)**2)\n",
        "segmentation_mask[dist_from_center < mask_radius] = 1 # Imposta a 1 dove c'√® la \"lesione\"\n",
        "\n",
        "# Combina l'immagine originale con la maschera di segmentazione\n",
        "# Sovrapponiamo la maschera sulla scala di blu per visualizzare l'area segmentata\n",
        "segmented_overlay = np.stack([original_image, original_image, original_image], axis=-1)\n",
        "segmented_overlay[segmentation_mask == 1, 0] = 255 # Rende rossa la zona segmentata\n",
        "segmented_overlay[segmentation_mask == 1, 1] = 0\n",
        "segmented_overlay[segmentation_mask == 1, 2] = 0\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(original_image, cmap='gray')\n",
        "plt.title('Immagine Originale')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(segmented_overlay)\n",
        "plt.title('Immagine Segmentata (Lesione Evidenziata)')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nSpiegazione della Segmentazione:\")\n",
        "print(\"In questo esempio concettuale, abbiamo simulato come un modello di IA (come MedSAM)\")\n",
        "print(\"potrebbe identificare e delineare una specifica area di interesse, come una lesione o\")\n",
        "print(\"una struttura anatomica. L'area rossa indica la regione 'segmentata' dal modello.\")\n",
        "print(\"Questo √® fondamentale per la quantificazione e la pianificazione diagnostica/terapeutica.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "wH06fQB3pdgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MedSAM: Segmentazione per immagini mediche"
      ],
      "metadata": {
        "id": "sFtt0sn8rHMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Data download\n",
        "url = \"https://drive.google.com/drive/folders/15hE_hEevLcIEq5Sf8hLotlMlA1KH_q06?usp=sharing\"\n",
        "import gdown\n",
        "gdown.download_folder(url, quiet=True)"
      ],
      "metadata": {
        "id": "_SPP_r8xv15c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Environment setup\n",
        "!pip install torch==2.4.1 torchvision==0.19.1 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install jax==0.4.33\n",
        "!pip install numpy==1.24\n",
        "\n",
        "!pip install open_clip_torch==2.23.0\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install grad_cam==1.4.6\n",
        "!pip install transformers==4.35.2"
      ],
      "metadata": {
        "id": "Fx-3QxHFrFaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get repo and install env\n",
        "!git clone https://github.com/HealthX-Lab/MedCLIP-SAMv2\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Torchvision version:\", torchvision.__version__)\n",
        "print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "import sys\n",
        "!{sys.executable} -m pip install opencv-python matplotlib\n",
        "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n"
      ],
      "metadata": {
        "id": "YIhNXzKRpeb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd MedCLIP-SAMv2/saliency_maps"
      ],
      "metadata": {
        "id": "50aMpswivJtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import torch\n",
        "import clip\n",
        "import cv2\n",
        "import sys\n",
        "import numpy as np\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from transformers import CLIPProcessor\n",
        "from scripts.plot import generate_shades_with_alpha, plot_text_with_colors, visualize_vandt_heatmap\n",
        "from scripts.methods import vision_heatmap_iba, text_heatmap_iba\n",
        "from transformers import AutoModel, AutoProcessor, AutoTokenizer\n",
        "from PIL import Image, ImageOps\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def plot(model, image, text, vbeta=0.1, vvar=1, vlayer=9, tbeta=0.1, tvar=1, tlayer=9):\n",
        "    # Preprocess image\n",
        "    image = Image.open(image).convert('RGB')\n",
        "    image_feat = processor(images=image, return_tensors=\"pt\")['pixel_values'].to(device) # 3*224*224\n",
        "    # Tokenize text\n",
        "    text_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)]).to(device)\n",
        "    text_words = tokenizer.convert_ids_to_tokens(text_ids[0].tolist())\n",
        "    # Train information bottleneck on image\n",
        "    print(\"Training M2IB on the image...\")\n",
        "    vmap = vision_heatmap_iba(text_ids, image_feat, model, vlayer, vbeta, vvar)\n",
        "    # Train information bottleneck on text\n",
        "    print(\"Training M2IB on the text...\")\n",
        "    tmap = text_heatmap_iba(text_ids, image_feat, model, tlayer, tbeta, tvar)\n",
        "    # Just for demonstration purposes\n",
        "    image_under = processor2(images=image, return_tensors=\"pt\", do_normalize=False)['pixel_values'][0].permute(1,2,0)\n",
        "    visualize_vandt_heatmap(tmap, vmap, text_words, image_under)\n",
        "    img = np.array(image)\n",
        "    vmap = cv2.resize(np.array(vmap),(img.shape[1],img.shape[0]),interpolation=cv2.INTER_NEAREST)*255\n",
        "    return vmap\n",
        "\n",
        "def overlay_segmentation(rgb_image, segmentation, alpha=0.4):\n",
        "    \"\"\"\n",
        "    Overlays a red segmentation mask on top of a grayscale-looking RGB image.\n",
        "\n",
        "    Parameters:\n",
        "        rgb_image (PIL.Image.Image): RGB image loaded with PIL (even if it looks grayscale).\n",
        "        segmentation (np.ndarray): 2D binary array (same width/height as image) where mask == 1 indicates the segmented area.\n",
        "        alpha (float): Transparency of the segmentation overlay.\n",
        "    \"\"\"\n",
        "    if not isinstance(rgb_image, Image.Image):\n",
        "        raise TypeError(\"rgb_image must be a PIL.Image.Image object\")\n",
        "\n",
        "    image_np = np.array(rgb_image).astype(np.float32) / 255.0  # Normalize to [0,1]\n",
        "    # print(image_np.shape)\n",
        "    # print(segmentation.shape)\n",
        "\n",
        "    if segmentation.shape != image_np.shape[:2]:\n",
        "        raise ValueError(\"Segmentation mask must have the same width and height as the image\")\n",
        "\n",
        "    # Create red overlay where segmentation is 1\n",
        "    red_mask = np.zeros_like(image_np)\n",
        "    red_mask[..., 0] = 1  # Set red channel\n",
        "\n",
        "    # Blend the red mask with the image where segmentation is 1\n",
        "    overlay = np.where(segmentation[..., None].astype(bool),\n",
        "                       (1 - alpha) * image_np + alpha * red_mask,\n",
        "                       image_np)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(overlay)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "NdR2AT_psSIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModel.from_pretrained(\"chuhac/BiomedCLIP-vit-bert-hf\", trust_remote_code=True).to(device)\n",
        "processor = AutoProcessor.from_pretrained(\"chuhac/BiomedCLIP-vit-bert-hf\", trust_remote_code=True)\n",
        "processor2 = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"chuhac/BiomedCLIP-vit-bert-hf\", trust_remote_code=True)"
      ],
      "metadata": {
        "id": "qjZmHGwps6UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "RgTmAL7oxzFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nCdLnwifxyvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!file /content/MedCLIP-SAMv2/saliency_maps/data/WhiteMatterHyperintensities.jpg\n"
      ],
      "metadata": {
        "id": "dGoDDvl7z1BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## Saliency Maps\n",
        "image_path = \"/content/MedCLIP-SAMv2/saliency_maps/data/Breast-Mammo/1-142.jpg\" # @param {type:\"string\"}\n",
        "text = \"segment the lesion in X-ray mammography\" # @param {type:\"string\"}\n",
        "vmap = plot(model, image_path, text)"
      ],
      "metadata": {
        "id": "oRzCghXhtNmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## Postprocessing\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=2,random_state=10)\n",
        "attn_weights = vmap / 255\n",
        "# Keep only high attention weight scores\n",
        "h, w = attn_weights.shape\n",
        "attn_wg_threshold = 0.75 #@param {type:\"slider\", min:0.1, max:1.0, step:0.05}\n",
        "filtered_attn_weights = attn_weights > attn_wg_threshold\n",
        "attn_weights = attn_weights*filtered_attn_weights\n",
        "image = cv2.resize(attn_weights, (256, 256),interpolation=cv2.INTER_NEAREST)\n",
        "flat_image = image.reshape(-1, 1)\n",
        "\n",
        "labels = kmeans.fit_predict(flat_image)\n",
        "\n",
        "segmented_image = labels.reshape(256, 256)\n",
        "\n",
        "centroids = kmeans.cluster_centers_.flatten()\n",
        "\n",
        "# Identify the background cluster (assuming it has the lowest centroid value)\n",
        "background_cluster = np.argmin(centroids)\n",
        "\n",
        "# Mark background pixels as 0 and foreground pixels as 1\n",
        "segmented_image = np.where(segmented_image == background_cluster, 0, 1)\n",
        "\n",
        "segmented_image = cv2.resize(segmented_image, (w,h),interpolation=cv2.INTER_NEAREST)\n",
        "segmented_image = segmented_image.astype(np.uint8)*255\n",
        "\n",
        "nb_blobs, im_with_separated_blobs, stats, _ = cv2.connectedComponentsWithStats(segmented_image)\n",
        "sizes = stats[:, cv2.CC_STAT_AREA]\n",
        "\n",
        "# Sort sizes (ignoring the background at index 0)\n",
        "sorted_sizes = sorted(sizes[1:], reverse=True)\n",
        "\n",
        "# Change this here if you want to segment more than one contour\n",
        "num_contours = 1\n",
        "\n",
        "# Determine the top K sizes\n",
        "top_k_sizes = sorted_sizes[:num_contours]\n",
        "\n",
        "im_result = np.zeros_like(im_with_separated_blobs)\n",
        "\n",
        "for index_blob in range(1, nb_blobs):\n",
        "    if sizes[index_blob] in top_k_sizes:\n",
        "        im_result[im_with_separated_blobs == index_blob] = 255\n",
        "\n",
        "segmented_image = im_result\n",
        "\n",
        "cv2.imwrite(\"/content/MedCLIP-SAMv2/postprocessed_map.png\", segmented_image)\n",
        "\n",
        "# plt.imshow(segmented_image,cmap=\"gray\")\n",
        "overlay_segmentation(rgb_image=Image.open(image_path).convert('RGB'), segmentation=segmented_image)"
      ],
      "metadata": {
        "id": "oJrM7soBtW9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RKo4UfQP4ZfP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}